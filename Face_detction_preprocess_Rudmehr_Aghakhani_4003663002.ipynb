{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#Introduction**\n",
    "**References**\n",
    "\n",
    " I)KazemiCVPR14\n",
    " \n",
    " II)Smile Detection in the Wild Based on Transfer Learning\n",
    "\n",
    "**Rudmehr Aghakhani StudentNO: 4003663002** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file2188_aligned_face.jpg\n",
      "file2345_aligned_face.jpg\n",
      "file2365_aligned_face.jpg\n",
      "file2395_aligned_face.jpg\n",
      "file2527_aligned_face.jpg\n",
      "file2576_aligned_face.jpg\n",
      "file2586_aligned_face.jpg\n",
      "file2594_aligned_face.jpg\n",
      "file2664_aligned_face.jpg\n",
      "file2669_face_1.jpg\n",
      "file2757_aligned_face.jpg\n",
      "file2853_aligned_face.jpg\n",
      "file2932_aligned_face.jpg\n",
      "file3064_aligned_face.jpg\n",
      "file3155_aligned_face.jpg\n",
      "file3369_face_0.jpg\n",
      "file3383_aligned_face.jpg\n",
      "file3389_aligned_face.jpg\n",
      "file3412_aligned_face.jpg\n",
      "file3582_aligned_face.jpg\n",
      "file3608_aligned_face.jpg\n",
      "file3670_aligned_face.jpg\n",
      "file3769_aligned_face.jpg\n",
      "file3786_aligned_face.jpg\n",
      "file3787_aligned_face.jpg\n",
      "file3824_aligned_face.jpg\n",
      "file3838_aligned_face.jpg\n",
      "file3862_aligned_face.jpg\n",
      "file3892_aligned_face.jpg\n",
      "file3924_aligned_face.jpg\n",
      "file3975_aligned_face.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths to folders\n",
    "input_folder = 'nonsmile_face_detection_first'  # Folder containing 'smile' and 'nonsmile' folders\n",
    "# output_folder_smile = 'smiled_face_detection_first_problematic_2_without_fault_0'\n",
    "output_folder_nonsmile = 'nonsmile_face_detection_first_without_fault'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "# os.makedirs(output_folder_smile, exist_ok=True)\n",
    "os.makedirs(output_folder_nonsmile, exist_ok=True)\n",
    "\n",
    "# Initialize Mediapipe face mesh detector\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)\n",
    "\n",
    "# Standard dimensions for resized faces\n",
    "standard_face_dim = (200, 200)  # Adjust as per requirement\n",
    "\n",
    "# Helper function for alignment\n",
    "def align_face(image, left_eye, right_eye, output_dim=(180, 192)):\n",
    "    # Calculate the center of the eyes and angle for alignment\n",
    "    eye_center = (left_eye + right_eye) /2.0\n",
    "    dy = right_eye[1] - left_eye[1]\n",
    "    dx = right_eye[0] - left_eye[0]\n",
    "    angle = np.degrees(np.arctan2(dy, dx))\n",
    "    \n",
    "    # Calculate scale\n",
    "    desired_distance = 70  # Distance between eyes in the aligned image\n",
    "    current_distance = np.sqrt(dx ** 2 + dy ** 2)\n",
    "    scale = desired_distance / current_distance\n",
    "    \n",
    "    # Rotation matrix for alignment\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(tuple(eye_center), angle, scale)\n",
    "    \n",
    "    # Apply the affine transformation\n",
    "    aligned_face = cv2.warpAffine(image, rotation_matrix, output_dim)\n",
    "    \n",
    "    return aligned_face\n",
    "\n",
    "# Process each folder (\"smile\" and \"nonsmile\")\n",
    "for category in [\"non_smile\"]:\n",
    "    input_path = os.path.join(input_folder, category)\n",
    "    output_path = output_folder_smile if category == \"smile\" else output_folder_nonsmile\n",
    "\n",
    "    # Loop through all images in the category folder\n",
    "    for filename in os.listdir(input_path):\n",
    "        img_path = os.path.join(input_path, filename)\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is not None:\n",
    "            # Resize image to improve consistency\n",
    "            max_dim = 2100\n",
    "            height, width = image.shape[:2]\n",
    "            if max(height, width) > max_dim:\n",
    "                scaling_factor = max_dim / float(max(height, width))\n",
    "                image = cv2.resize(image, (int(width * scaling_factor), int(height * scaling_factor)))\n",
    "\n",
    "            # Convert to RGB (required for Mediapipe)\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Detect facial landmarks\n",
    "            result = face_mesh.process(rgb_image)\n",
    "            if result.multi_face_landmarks:\n",
    "                for landmarks in result.multi_face_landmarks:\n",
    "                    # Extract eye positions\n",
    "                    left_eye = np.array([landmarks.landmark[33].x * width, landmarks.landmark[33].y * height])\n",
    "                    right_eye = np.array([landmarks.landmark[263].x * width, landmarks.landmark[263].y * height])\n",
    "\n",
    "                    # Align the face based on eye positions\n",
    "                    aligned_face = align_face(image, left_eye, right_eye, output_dim=standard_face_dim)\n",
    "                    \n",
    "                    # Save the aligned face\n",
    "                    face_filename = f\"{os.path.splitext(filename)[0]}_aligned_face.jpg\"\n",
    "                    cv2.imwrite(os.path.join(output_path, face_filename), aligned_face)\n",
    "            else:\n",
    "                print(filename)\n",
    "        else:\n",
    "            print(f\"Could not open image: {img_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problematic_3_non_smile\\non_smile\\file2188.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2188.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2188.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2188.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2345.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2345.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2345.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2345.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2345.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2345.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2345.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2345.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2345.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2345.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2345.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2345.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2365.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2365.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2365.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2365.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2365.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2365.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2365.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2365.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2576.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2576.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2576.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2576.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2576.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2576.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2889.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2889.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2889.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2889.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2889.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2889.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2889.jpg\n",
      "problematic_3_non_smile\\non_smile\\file2889.jpg\n",
      "problematic_3_non_smile\\non_smile\\file3608.jpg\n",
      "problematic_3_non_smile\\non_smile\\file3608.jpg\n",
      "problematic_3_non_smile\\non_smile\\file3608.jpg\n",
      "problematic_3_non_smile\\non_smile\\file3608.jpg\n",
      "problematic_3_non_smile\\non_smile\\file3838.jpg\n",
      "problematic_3_non_smile\\non_smile\\file3838.jpg\n",
      "problematic_3_non_smile\\non_smile\\file3838.jpg\n",
      "problematic_3_non_smile\\non_smile\\file3838.jpg\n",
      "problematic_3_non_smile\\non_smile\\file3838.jpg\n",
      "problematic_3_non_smile\\non_smile\\file3838.jpg\n",
      "problematic_3_non_smile\\non_smile\\file3838.jpg\n",
      "problematic_3_non_smile\\non_smile\\file3838.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths to folders\n",
    "input_folder = 'problematic_3_non_smile'  # Folder containing 'smile' and 'nonsmile' folders\n",
    "output_folder_smile = 'nonsmile_face_detection_non_smile_problematic_3'\n",
    "output_folder_nonsmile = 'nonsmile_face_detection_non_smile_problematic_3'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "# os.makedirs(output_folder_smile, exist_ok=True)\n",
    "os.makedirs(output_folder_nonsmile, exist_ok=True)\n",
    "\n",
    "# Load the Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Minimum face size (adjust based on expected face sizes)\n",
    "min_face_size = (30, 30)  # Width, Height (pixels)\n",
    "\n",
    "# Process each folder (\"smile\" and \"nonsmile\")\n",
    "for category in [\"non_smile\"]:\n",
    "    input_path = os.path.join(input_folder, category)\n",
    "    output_path = output_folder_smile if category == \"smile\" else output_folder_nonsmile\n",
    "\n",
    "    # Loop through all images in the category folder\n",
    "    for filename in os.listdir(input_path):\n",
    "        img_path = os.path.join(input_path, filename)\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        if image is not None:\n",
    "            \n",
    "            # Resize image for consistency (if images vary greatly in size)\n",
    "            max_dim = 800  # Resize to a max dimension of 800 (maintain aspect ratio)\n",
    "            height, width = image.shape[:2]\n",
    "            if max(height, width) > max_dim:\n",
    "                scaling_factor = max_dim / float(max(height, width))\n",
    "                image = cv2.resize(image, (int(width * scaling_factor), int(height * scaling_factor)))\n",
    "\n",
    "            # Convert to grayscale for face detection\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Detect faces with specified minimum size\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray, scaleFactor=1.001, minNeighbors=5, minSize=min_face_size\n",
    "            )\n",
    "            if len(faces) == 0:\n",
    "                print(filename)\n",
    "            # Save each detected face to the respective output folder\n",
    "            for i, (x, y, w, h) in enumerate(faces):\n",
    "                # Filter out detections that are too wide or too tall (likely not a face)\n",
    "                aspect_ratio = w / h\n",
    "                if 0.75 < aspect_ratio < 1.3:  # Accept only nearly square faces\n",
    "                    face = image[y:y+h, x:x+w]\n",
    "                    face_filename = f\"{os.path.splitext(filename)[0]}_face_{i}.jpg\"\n",
    "                    print(img_path)\n",
    "                    # Optionally resize cropped face to a standard size\n",
    "                    face_resized = cv2.resize(face, (150, 150))  # Resize to 150x150 for uniformity\n",
    "                    \n",
    "                    cv2.imwrite(os.path.join(output_path, face_filename), face_resized)\n",
    "                    print(img_path)\n",
    "                else:\n",
    "                    print(f\"Skipped non-face detection in {filename}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Could not open image: {img_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face landmarks detected in file2365_face_0.jpg\n",
      "No face landmarks detected in file2669_face_1.jpg\n",
      "No face landmarks detected in file3369_face_0.jpg\n",
      "No face landmarks detected in file3838_face_1.jpg\n",
      "No face landmarks detected in file3932_aligned_face_aligned_face.jpg\n",
      "All images aligned and resized successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "input_folder = 'E:\\\\workspace\\\\Final_detection\\\\non_smile'\n",
    "output_folder = 'non_smiled_image_processed11'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Initialize Mediapipe face mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)\n",
    "\n",
    "# Desired dimensions\n",
    "output_dim = (224, 224)\n",
    "\n",
    "# Helper function for alignment\n",
    "def align_face(image, left_eye, right_eye, output_dim=(224, 224)):\n",
    "    # Calculate the center of the eyes and angle for alignment\n",
    "    eye_center = (left_eye + right_eye) / 2.0\n",
    "    dy = right_eye[1] - left_eye[1]\n",
    "    dx = right_eye[0] - left_eye[0]\n",
    "    angle = np.degrees(np.arctan2(dy, dx))\n",
    "    \n",
    "    # Calculate scale\n",
    "    desired_distance = 112  # Half of the output width for symmetry\n",
    "    current_distance = np.sqrt(dx ** 2 + dy ** 2)\n",
    "    scale = desired_distance / current_distance\n",
    "    \n",
    "    # Rotation matrix for alignment\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(tuple(eye_center), angle, scale)\n",
    "    \n",
    "    # Apply the affine transformation to align face\n",
    "    aligned_face = cv2.warpAffine(image, rotation_matrix, (output_dim[0], output_dim[1]))\n",
    "    \n",
    "    return aligned_face\n",
    "\n",
    "# Process each image in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    if image is not None:\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        # Convert to RGB for Mediapipe\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect facial landmarks\n",
    "        result = face_mesh.process(rgb_image)\n",
    "        \n",
    "        if result.multi_face_landmarks:\n",
    "            for landmarks in result.multi_face_landmarks:\n",
    "                # Extract eye positions for alignment\n",
    "                left_eye = np.array([landmarks.landmark[33].x * width, landmarks.landmark[33].y * height])\n",
    "                right_eye = np.array([landmarks.landmark[263].x * width, landmarks.landmark[263].y * height])\n",
    "                \n",
    "                # Align and resize the face\n",
    "                aligned_face = align_face(image, left_eye, right_eye, output_dim=output_dim)\n",
    "                \n",
    "                # Save the aligned and resized face\n",
    "                output_path = os.path.join(output_folder, filename)\n",
    "                cv2.imwrite(output_path, aligned_face)\n",
    "        else:\n",
    "            print(f\"No face landmarks detected in {filename}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Could not open image: {filename}\")\n",
    "\n",
    "print(\"All images aligned and resized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized file2669_face_1.jpg and saved to resized_images_224_224\n",
      "Resized file3838_face_1.jpg and saved to resized_images_224_224\n",
      "Resized file3932_aligned_face_aligned_face.jpg and saved to resized_images_224_224\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def resize_images_in_folder(input_folder, output_folder):\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Iterate through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):  # Add more formats if needed\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            # Read the image\n",
    "            img = cv2.imread(input_path)\n",
    "            if img is not None:\n",
    "                # Resize the image\n",
    "                resized_img = cv2.resize(img, (224, 224))\n",
    "                # Save the resized image\n",
    "                cv2.imwrite(output_path, resized_img)\n",
    "                print(f'Resized {filename} and saved to {output_folder}')\n",
    "            else:\n",
    "                print(f'Failed to read {filename}')\n",
    "\n",
    "# Specify your input and output folders\n",
    "input_folder = 'resized_images'\n",
    "output_folder = 'resized_images_224_224'\n",
    "\n",
    "resize_images_in_folder(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files are missing in folder2 from folder1.\n",
      "\n",
      "Files in folder2 but missing in folder1:\n",
      "file2188\n",
      "file2345\n",
      "file2365\n",
      "file2576\n",
      "file2889\n",
      "file3608\n",
      "file3838\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Paths to the folders\n",
    "folder1 = 'nonsmile_face_detection_first_without_fault'  # Folder with 1838 images\n",
    "folder2 = 'E:\\\\workspace\\\\kaggle-genki4k\\\\non_smile'  # Folder with 1832 images\n",
    "\n",
    "# Regular expression to extract the file number from filenames starting with 'file'\n",
    "file_pattern = re.compile(r\"file(\\d+)\")\n",
    "\n",
    "# Function to extract file numbers from filenames in a folder\n",
    "def get_file_numbers(folder_path):\n",
    "    file_numbers = set()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        match = file_pattern.match(filename)\n",
    "        if match:\n",
    "            file_number = int(match.group(1))\n",
    "            file_numbers.add(file_number)\n",
    "    return file_numbers\n",
    "\n",
    "# Extract file numbers from each folder\n",
    "file_numbers_folder1 = get_file_numbers(folder1)\n",
    "file_numbers_folder2 = get_file_numbers(folder2)\n",
    "\n",
    "# Find missing file numbers in each folder\n",
    "missing_in_folder2 = file_numbers_folder1 - file_numbers_folder2\n",
    "missing_in_folder1 = file_numbers_folder2 - file_numbers_folder1\n",
    "\n",
    "# Output the missing files\n",
    "if missing_in_folder2:\n",
    "    print(\"Files in folder1 but missing in folder2:\")\n",
    "    for file_number in sorted(missing_in_folder2):\n",
    "        print(f\"file{file_number}\")\n",
    "else:\n",
    "    print(\"No files are missing in folder2 from folder1.\")\n",
    "\n",
    "if missing_in_folder1:\n",
    "    print(\"\\nFiles in folder2 but missing in folder1:\")\n",
    "    for file_number in sorted(missing_in_folder1):\n",
    "        print(f\"file{file_number}\")\n",
    "else:\n",
    "    print(\"\\nNo files are missing in folder1 from folder2.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate files found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Path to the folder\n",
    "folder_path = 'nonsmile_face_detection_first_without_fault'\n",
    "\n",
    "# Regular expression to extract the file number from filenames starting with 'filename'\n",
    "file_pattern = re.compile(r\"filename(\\d+)\")\n",
    "\n",
    "# Track seen numbers and duplicates\n",
    "seen_numbers = set()\n",
    "duplicate_files = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    match = file_pattern.match(filename)\n",
    "    if match:\n",
    "        file_number = int(match.group(1))\n",
    "        \n",
    "        # Check if the file number is already seen\n",
    "        if file_number in seen_numbers:\n",
    "            duplicate_files.append(filename)\n",
    "        else:\n",
    "            seen_numbers.add(file_number)\n",
    "\n",
    "# Output the results\n",
    "if duplicate_files:\n",
    "    print(\"Duplicate files found:\")\n",
    "    for duplicate in duplicate_files:\n",
    "        print(duplicate)\n",
    "else:\n",
    "    print(\"No duplicate files found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#7***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise reduction applied to all images successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "input_folder = 'non_smiled_image_processed11'       # Path to the folder containing original images\n",
    "output_folder = 'non_smiled_image_processed_denoised'     # Path to save denoised images\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process each image in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    if image is not None:\n",
    "        # Apply Non-Local Means Denoising\n",
    "        denoised_image = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n",
    "\n",
    "        # Save the denoised image to the output folder\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        cv2.imwrite(output_path, denoised_image)\n",
    "    else:\n",
    "        print(f\"Could not open image: {filename}\")\n",
    "\n",
    "print(\"Noise reduction applied to all images successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
